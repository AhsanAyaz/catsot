---
phase: 01-foundation
plan: 04
type: execute
wave: 2
depends_on: ["01-01", "01-02"]
files_modified:
  - modules/04-context-engineering/README.md
  - modules/04-context-engineering/demonstration.md
  - modules/04-context-engineering/exercise.md
  - modules/04-context-engineering/prompt-templates/structured-template.md
  - modules/04-context-engineering/prompt-templates/few-shot-examples.md
  - modules/04-context-engineering/solutions/solution.md
autonomous: true

must_haves:
  truths:
    - "Demonstration shows system instructions and few-shot examples"
    - "Participants understand prompt structure: Role + Task + Context + Format"
    - "Exercise teaches XML tags or Markdown for structured prompts"
    - "Comparison shows unstructured vs structured prompt performance"
  artifacts:
    - path: "modules/04-context-engineering/README.md"
      provides: "Module overview for context engineering"
      min_lines: 20
    - path: "modules/04-context-engineering/demonstration.md"
      provides: "System instructions and prompt structure demo"
      min_lines: 45
    - path: "modules/04-context-engineering/prompt-templates/structured-template.md"
      provides: "Reusable prompt template pattern"
      min_lines: 20
    - path: "modules/04-context-engineering/solutions/solution.md"
      provides: "Before/after examples with explanations"
      min_lines: 40
  key_links:
    - from: "modules/04-context-engineering/exercise.md"
      to: "prompt-templates/"
      via: "references template patterns"
      pattern: "prompt-templates/"
    - from: "modules/04-context-engineering/demonstration.md"
      to: "AI Studio system instructions"
      via: "configuration in settings"
      pattern: "system.*instruction"
---

<objective>
Create Module 04: Context Engineering - teaching participants how to craft effective prompts using structure, system instructions, and few-shot examples.

Purpose: This module transforms participants from basic prompters to context engineers who understand how to guide LLM behavior systematically. Builds on Modules 01-02 by showing how to improve prompt quality beyond just asking questions.

Output: Complete module with prompt templates, demonstration of system instructions and few-shot patterns, hands-on exercise refactoring unstructured prompts, and solutions showing before/after improvements.
</objective>

<execution_context>
@/home/ahsan/.claude/get-shit-done/workflows/execute-plan.md
@/home/ahsan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@/home/ahsan/projects/code-at-speed-of-thought/.planning/PROJECT.md
@/home/ahsan/projects/code-at-speed-of-thought/.planning/ROADMAP.md
@/home/ahsan/projects/code-at-speed-of-thought/.planning/STATE.md
@/home/ahsan/projects/code-at-speed-of-thought/.planning/phases/01-foundation/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Create Module 04 structure, README, and prompt templates</name>
  <files>
    modules/04-context-engineering/README.md
    modules/04-context-engineering/prompt-templates/structured-template.md
    modules/04-context-engineering/prompt-templates/few-shot-examples.md
    modules/04-context-engineering/solutions/.gitkeep
  </files>
  <action>
Create module directory structure, README, and two reusable prompt templates following RESEARCH.md patterns.

**README.md content:**
- Module title: "Module 04: Context Engineering"
- Duration: 20 minutes
- Learning Objectives:
  * Understand the Role + Task + Context + Format pattern for effective prompts
  * Use system instructions to set model behavior
  * Apply few-shot examples to teach desired output patterns
  * Recognize when to use structured delimiters (XML tags, Markdown)
- Prerequisites:
  * Completion of Module 01 (AI Studio basics)
  * Completion of Module 02 (Structured output helpful but not required)
  * Understanding that prompts are instructions, not just questions
- Overview explaining context engineering vs basic prompting:
  * Basic prompting: "Tell me about X"
  * Context engineering: System instructions + structured prompt + examples
- Reference to demonstration.md, exercise.md, and prompt-templates/

**prompt-templates/structured-template.md content:**

```markdown
# Structured Prompt Template

## Pattern: Role + Task + Context + Format

Use this template to create effective, structured prompts.

### Template

\`\`\`
<system_instruction>
You are a [ROLE - e.g., code review assistant, technical writer].
[BEHAVIOR - e.g., Provide constructive feedback, Keep explanations concise].
</system_instruction>

<task>
[WHAT TO DO - e.g., Analyze the following code, Summarize this article]
</task>

<context>
[ADDITIONAL INFO - e.g., Target audience is beginners, Focus on security issues]
</context>

<input>
[THE ACTUAL CONTENT TO PROCESS]
</input>

<output_format>
[HOW TO STRUCTURE RESPONSE - e.g., JSON, bullet points, step-by-step]
</output_format>
\`\`\`

### Example Usage

**Unstructured (worse):**
```
Review this code and tell me if there are any problems and also suggest improvements and make it better.

def login(username, password):
    if username == "admin" and password == "admin":
        return True
    return False
```

**Structured (better):**
```
<system_instruction>
You are a security-focused code review assistant.
Identify security vulnerabilities and suggest improvements.
Keep feedback actionable and specific.
</system_instruction>

<task>
Review the following code for security issues and best practices.
</task>

<context>
This is authentication code for a web application.
Target audience: intermediate developers.
</context>

<input>
def login(username, password):
    if username == "admin" and password == "admin":
        return True
    return False
</input>

<output_format>
{
  "security_issues": ["issue 1", "issue 2"],
  "severity": "high|medium|low",
  "recommendations": ["action 1", "action 2"]
}
</output_format>
```

### Why This Works

- **Clear role:** Model knows what perspective to take
- **Explicit task:** No ambiguity about what to do
- **Relevant context:** Model understands constraints and audience
- **Defined format:** Output is predictable and parseable
- **XML tags:** Help model parse different sections clearly

### When to Use

- Complex tasks requiring specific behavior
- Consistent output format needed
- Multiple pieces of context must be provided
- Combining with structured output (Module 02)
```

**prompt-templates/few-shot-examples.md content:**

```markdown
# Few-Shot Examples Pattern

## What Are Few-Shot Examples?

Showing the model 2-3 examples of desired input → output before asking it to process new input.

## Template

\`\`\`
<system_instruction>
You are a [ROLE].
</system_instruction>

<examples>
Example 1:
Input: [example input 1]
Output: [desired output 1]

Example 2:
Input: [example input 2]
Output: [desired output 2]

Example 3 (optional):
Input: [example input 3]
Output: [desired output 3]
</examples>

<task>
Now process this new input following the same pattern:
Input: [actual input to process]
</task>
\`\`\`

## Example Usage: Sentiment Analysis

**Without few-shot (less reliable):**
```
Analyze the sentiment of this review: "The product is okay but shipping was slow"
```

**With few-shot (more reliable):**
```
<system_instruction>
You are a sentiment analysis assistant.
Classify customer reviews into positive, negative, or neutral.
</system_instruction>

<examples>
Example 1:
Review: "Amazing product! Fast delivery and great quality."
Sentiment: positive
Confidence: 95%
Reason: Strong positive language, no negatives

Example 2:
Review: "Terrible experience. Product broke after one day."
Sentiment: negative
Confidence: 98%
Reason: Strong negative language, product failure

Example 3:
Review: "It works as described. Nothing special."
Sentiment: neutral
Confidence: 85%
Reason: Functional but no enthusiasm or complaints
</examples>

<task>
Now analyze this review following the same pattern:
Review: "The product is okay but shipping was slow"
</task>
```

**Expected output:**
```
Sentiment: neutral
Confidence: 80%
Reason: Mixed signals - product is acceptable, but shipping complaint
```

## Why This Works

- **Pattern learning:** Model sees the desired output structure
- **Consistency:** Examples teach format, tone, and detail level
- **Edge cases:** Examples can show how to handle tricky cases
- **Less ambiguity:** Model doesn't have to guess what you want

## When to Use

- Specific output format required (especially with unusual structure)
- Consistent tone/style needed
- Task is uncommon or domain-specific
- Zero-shot results are inconsistent

## How Many Examples?

- **2-3 examples:** Sweet spot for most tasks
- **1 example:** Often not enough to establish pattern
- **5+ examples:** Diminishing returns, uses more tokens

## Combining with Structured Output

Few-shot examples + JSON Schema = highly consistent results.

Example: Show 2-3 JSON outputs in examples, then use schema to enforce structure.
```
  </action>
  <verify>
Verify all files exist:
```bash
ls -R modules/04-context-engineering/
```
Expected: README.md, prompt-templates/structured-template.md, prompt-templates/few-shot-examples.md, solutions/.gitkeep

Verify templates include examples:
```bash
cat modules/04-context-engineering/prompt-templates/*.md | grep -i "example" | wc -l
```
Expected: At least 10 mentions

Verify README has learning objectives:
```bash
cat modules/04-context-engineering/README.md | grep -i "learning objective" -A 6 | wc -l
```
Expected: At least 5 lines
  </verify>
  <done>
Module 04 structure exists with README containing learning objectives, two prompt templates (structured Role+Task+Context+Format and few-shot examples patterns) with before/after comparisons showing why structured prompts perform better.
  </done>
</task>

<task type="auto">
  <name>Create demonstration showing system instructions and few-shot workflow</name>
  <files>modules/04-context-engineering/demonstration.md</files>
  <action>
Create instructor demonstration guide showing AI Studio's system instruction feature and few-shot prompting, using code from RESEARCH.md.

**Content structure (5-7 minute demonstration):**

**Introduction (1 minute):**
- The progression: Module 01 (basic prompts) → Module 02 (structured output) → Module 04 (context engineering)
- What is context engineering: Systematically guiding model behavior through structure and examples
- Two key techniques: System instructions + Few-shot examples

**Step-by-step demonstration (5-6 minutes):**

1. **Baseline: Unstructured prompt**
   - Action: Create new prompt in AI Studio
   - Action: Enter: "Review this code and suggest improvements: def calc(a,b): return a+b"
   - Action: Run
   - Expected result: Generic response, inconsistent format
   - Point out: Works, but format varies, may miss specific concerns
   - [Screenshot: Unstructured prompt response]

2. **Add system instructions**
   - Action: Click "System instructions" (usually in settings/config panel)
   - Expected result: System instruction text field appears
   - Action: Enter system instruction:
     ```
     You are a code review assistant. Provide constructive feedback on code quality,
     focusing on readability, best practices, and potential bugs. Keep feedback concise
     and actionable.
     ```
   - Action: Re-run same prompt
   - Expected result: More focused response aligned with system instruction
   - Point out: System instruction sets the role and behavior
   - [Screenshot: System instructions panel]

3. **Structure the prompt with XML tags**
   - Action: Keep system instruction, restructure prompt:
     ```
     <code>
     def calc(a,b):
         return a+b
     </code>

     <focus_areas>
     - Naming clarity
     - Type hints
     - Documentation
     </focus_areas>
     ```
   - Action: Run
   - Expected result: Response addresses specific focus areas
   - Point out: XML tags help model parse different sections

4. **Add few-shot examples**
   - Action: Update prompt to include examples:
     ```
     Example 1:
     Code: def process(data): return data * 2
     Feedback:
     - Function name 'process' is too generic. Use 'double_value' instead.
     - Missing type hints. Add: def double_value(data: int) -> int
     - Missing docstring.

     Example 2:
     Code: def calculate_total(prices: list[float]) -> float: return sum(prices)
     Feedback:
     - Good descriptive name
     - Type hints present ✓
     - Consider adding docstring explaining purpose
     - Handle empty list case

     Now review this code:
     <code>
     def calc(a, b):
         return a + b
     </code>
     ```
   - Action: Run
   - Expected result: Response follows example format (bullet points, similar structure)
   - Point out: Examples teach the desired output pattern
   - [Screenshot: Few-shot prompt with consistent output]

5. **Combine with structured output**
   - Action: Enable JSON mode (from Module 02)
   - Action: Add schema:
     ```json
     {
       "type": "object",
       "properties": {
         "issues": {
           "type": "array",
           "description": "List of code quality issues found",
           "items": {"type": "string"}
         },
         "severity": {
           "type": "string",
           "description": "Overall severity: low, medium, or high"
         },
         "improved_code": {
           "type": "string",
           "description": "Improved version of the code"
         }
       }
     }
     ```
   - Action: Run (keep system instruction and few-shot examples)
   - Expected result: JSON output with consistent structure + context-aware content
   - Point out: System instructions + few-shot + structured output = powerful combination

6. **Export as template**
   - Action: Save this prompt in AI Studio (if feature available)
   - Show: How to reuse for future code reviews
   - Explain: This is the template pattern - reusable context engineering

**Key talking points:**
- **System instructions:** Set role and behavior at model level (applies to all turns)
- **Few-shot examples:** Teach output format through demonstration
- **Structured delimiters:** XML tags or Markdown help model parse sections
- **Combinable:** Context engineering works with all previous module techniques
- **Templates:** Save well-engineered prompts for reuse

**Anti-pattern to show:**
- Long rambling prompt without structure
- Result: Inconsistent or unfocused output
- Fix: Apply Role + Task + Context + Format pattern
  </action>
  <verify>
Verify demonstration.md contains required sections:

```bash
cat modules/04-context-engineering/demonstration.md | grep -E "(Step [0-9]:|Action:|Expected result:|Screenshot:)" | wc -l
```
Expected: At least 20 matches (6 steps with multiple actions)

Verify system instructions feature is demonstrated:
```bash
cat modules/04-context-engineering/demonstration.md | grep -i "system instruction" | wc -l
```
Expected: At least 5 mentions

Verify few-shot pattern is shown:
```bash
cat modules/04-context-engineering/demonstration.md | grep -i "example" | wc -l
```
Expected: At least 8 mentions
  </verify>
  <done>
Demonstration guide exists with complete walkthrough showing progression from unstructured to fully context-engineered prompt: system instructions + XML tags + few-shot examples + structured output combination, following RESEARCH.md code review example.
  </done>
</task>

<task type="auto">
  <name>Create hands-on exercise refactoring unstructured prompts</name>
  <files>
    modules/04-context-engineering/exercise.md
    modules/04-context-engineering/solutions/solution.md
  </files>
  <action>
Create hands-on exercise where participants refactor a poorly-structured prompt using context engineering techniques.

**exercise.md content (10-13 minutes):**

**Your Task:**
Take unstructured prompts and improve them using context engineering techniques (system instructions, structure, few-shot examples).

**Setup (1 minute):**
1. Open aistudio.google.com
2. Create new prompt
3. Have the prompt templates open for reference

**Option A: Guided Challenge (For beginners)**

**Challenge:** Refactor a customer feedback analysis prompt

Step 1: Test the unstructured version (2 minutes)
- Copy this prompt into AI Studio:
  ```
  Analyze this customer feedback and tell me if it's good or bad and what
  they're talking about and give me a score: "The new dashboard is confusing
  but the support team was helpful"
  ```
- Run it and observe the response
- Note: Response works but format is inconsistent, unclear what score means

Step 2: Add system instructions (3 minutes)
- Click "System instructions"
- Add:
  ```
  You are a customer feedback analyzer. Classify sentiment, identify topics,
  and assess whether action is required. Be consistent in your output format.
  ```
- Re-run and compare results
- Success criteria: Response is more focused on analysis

Step 3: Structure the prompt (3 minutes)
- Restructure using XML tags (reference `prompt-templates/structured-template.md`):
  ```
  <system_instruction>
  You are a customer feedback analyzer.
  Classify sentiment, identify topics, and assess whether action is required.
  </system_instruction>

  <feedback>
  The new dashboard is confusing but the support team was helpful
  </feedback>

  <output_format>
  - Sentiment: [positive/negative/neutral]
  - Topic: [main subject]
  - Confidence: [0-100]
  - Action required: [yes/no]
  - Reason: [brief explanation]
  </output_format>
  ```
- Run and compare to previous versions
- Success criteria: Output follows the specified format

Step 4: Add few-shot examples (optional, 3 minutes)
- Add 2 examples before your feedback (reference `prompt-templates/few-shot-examples.md`):
  ```
  Example 1:
  Feedback: "Amazing product! Fast delivery."
  Analysis:
  - Sentiment: positive
  - Topic: product and shipping
  - Confidence: 95
  - Action required: no
  - Reason: Strong positive sentiment, no issues

  Example 2:
  Feedback: "Terrible experience. Product broke after one day."
  Analysis:
  - Sentiment: negative
  - Topic: product quality
  - Confidence: 98
  - Action required: yes
  - Reason: Product failure, high severity

  Now analyze this feedback:
  [your feedback here]
  ```
- Run and observe consistency
- Success criteria: Output matches example format exactly

**Option B: Independent Challenge (For advanced users)**

Goal: Choose a use case relevant to your work and engineer an effective context for it.

Use case ideas:
- Technical documentation generator (code → docs)
- Email tone analyzer (professional, friendly, aggressive)
- Bug report triaging (priority, affected component, suggested owner)

Requirements:
- Write an unstructured version first (baseline)
- Add system instructions defining role and behavior
- Structure the prompt with XML tags or Markdown
- Include 2-3 few-shot examples showing desired output
- Test with multiple inputs to verify consistency
- (Optional) Combine with structured output from Module 02

Solution: See `solutions/solution.md` for reference

**For Fast Finishers: Going Further**

Advanced challenges:
- Combine context engineering + structured output (JSON Schema)
- Create a reusable template for your use case
- Test edge cases (empty input, very long input, ambiguous cases)
- Compare zero-shot vs few-shot performance
- Export the API code to see how system instructions translate

**Success Criteria:**
- [ ] You have refactored at least one unstructured prompt
- [ ] You understand the Role + Task + Context + Format pattern
- [ ] You have used system instructions in AI Studio
- [ ] You have tested few-shot examples
- [ ] You can explain why structured prompts perform better

Duration: ⏱️ Expected: 12 minutes | Buffer: 3 minutes

**solutions/solution.md content:**

# Module 04 Solution

## Customer Feedback Analysis - Complete Refactor

### Version 1: Unstructured (Baseline)

**Prompt:**
```
Analyze this customer feedback and tell me if it's good or bad and what they're talking about and give me a score: "The new dashboard is confusing but the support team was helpful"
```

**Issues:**
- No clear role for the model
- Multiple tasks crammed into one sentence
- Unclear what "score" means
- No output format specified
- Results will be inconsistent across different feedback

### Version 2: With System Instructions

**System Instruction:**
```
You are a customer feedback analyzer. Classify sentiment, identify topics, and assess whether action is required. Be consistent in your output format.
```

**Prompt:**
```
Analyze this customer feedback: "The new dashboard is confusing but the support team was helpful"
```

**Improvement:**
- Clear role defined
- Model knows to focus on sentiment, topics, action
- Simplified prompt (role is in system instruction)

### Version 3: Structured with XML Tags

```
<system_instruction>
You are a customer feedback analyzer.
Classify sentiment, identify topics, and assess whether action is required.
</system_instruction>

<feedback>
The new dashboard is confusing but the support team was helpful
</feedback>

<output_format>
- Sentiment: [positive/negative/neutral]
- Topic: [main subject]
- Confidence: [0-100]
- Action required: [yes/no]
- Reason: [brief explanation]
</output_format>
```

**Expected Response:**
```
- Sentiment: neutral
- Topic: dashboard UX and customer support
- Confidence: 85
- Action required: yes
- Reason: Dashboard usability issue identified, though support mitigated impact
```

**Improvement:**
- Clear sections (feedback separate from format specification)
- Explicit output structure
- Model can parse different parts easily

### Version 4: With Few-Shot Examples (Best)

```
<system_instruction>
You are a customer feedback analyzer.
Classify sentiment, identify topics, and assess whether action is required.
Be consistent in your output format.
</system_instruction>

<examples>
Example 1:
Feedback: "Amazing product! Fast delivery and great quality."
Analysis:
- Sentiment: positive
- Topic: product quality and shipping
- Confidence: 95
- Action required: no
- Reason: Strong positive sentiment across multiple areas

Example 2:
Feedback: "Terrible experience. Product broke after one day and support was unhelpful."
Analysis:
- Sentiment: negative
- Topic: product quality and customer support
- Confidence: 98
- Action required: yes
- Reason: Critical product failure plus poor support experience

Example 3:
Feedback: "It works as described. Nothing special."
Analysis:
- Sentiment: neutral
- Topic: product functionality
- Confidence: 80
- Action required: no
- Reason: Meets expectations but no enthusiasm or complaints
</examples>

<task>
Now analyze this feedback following the same pattern:
Feedback: "The new dashboard is confusing but the support team was helpful"
</task>
```

**Expected Response (highly consistent):**
```
Analysis:
- Sentiment: neutral
- Topic: dashboard UX and customer support
- Confidence: 85
- Action required: yes
- Reason: Mixed feedback with UX issue (dashboard confusion) balanced by positive support experience; dashboard issue warrants follow-up
```

**Improvement:**
- Examples teach the exact format
- Examples show how to handle edge cases (mixed sentiment)
- Output will be highly consistent across different feedback

### Version 5: With Structured Output (Production-Ready)

Combine Version 4 with JSON Schema from Module 02:

**Schema:**
```json
{
  "type": "object",
  "properties": {
    "sentiment": {
      "type": "string",
      "description": "Overall sentiment classification",
      "enum": ["positive", "negative", "neutral"]
    },
    "topic": {
      "type": "string",
      "description": "Main subject of the feedback"
    },
    "confidence": {
      "type": "number",
      "description": "Confidence score from 0-100",
      "minimum": 0,
      "maximum": 100
    },
    "action_required": {
      "type": "boolean",
      "description": "Whether this feedback requires follow-up action"
    },
    "reason": {
      "type": "string",
      "description": "Brief explanation of the analysis"
    }
  },
  "required": ["sentiment", "topic", "confidence", "action_required", "reason"]
}
```

**Result:** Guaranteed JSON structure + context-engineered content quality

## Key Learnings

**Progression:**
1. Unstructured → works but inconsistent
2. System instructions → defines role and behavior
3. Structured tags → clear sections and format
4. Few-shot examples → teaches exact output pattern
5. + Structured output → production-ready consistency

**Best Practices:**
- **Start with role:** Define who the model should be (analyzer, assistant, reviewer)
- **Separate concerns:** Use XML tags to distinguish input, context, output format
- **Show, don't just tell:** Few-shot examples are more powerful than descriptions
- **Iterate:** Test with multiple inputs, refine based on results
- **Combine techniques:** System instructions + structure + examples + schema = best results

**When to Use Each:**
- **System instructions:** Always (defines baseline behavior)
- **Structured tags:** When multiple types of input/context (code + requirements + constraints)
- **Few-shot examples:** When specific output format or style required
- **Structured output:** When JSON/parseable data needed

## Common Issues

**Issue:** "Output format keeps changing"
**Solution:** Add few-shot examples showing exact format

**Issue:** "Model ignores my instructions"
**Solution:** Move role/behavior to system instructions, use XML tags to separate sections

**Issue:** "Response is too generic"
**Solution:** Add more specific context (target audience, focus areas, constraints)

**Issue:** "Few-shot examples make prompt too long"
**Solution:** Use 2-3 examples max. If more needed, consider fine-tuning instead.

## API Code (Python)

```python
from google import genai
from google.genai import types

client = genai.Client(api_key="YOUR_API_KEY")

# System instruction
system_inst = """
You are a customer feedback analyzer.
Classify sentiment, identify topics, and assess whether action is required.
Be consistent in your output format.
"""

# Few-shot examples + task
prompt = """
<examples>
Example 1:
Feedback: "Amazing product! Fast delivery."
Analysis:
- Sentiment: positive
- Topic: product and shipping
- Confidence: 95
- Action required: no

Example 2:
Feedback: "Product broke after one day."
Analysis:
- Sentiment: negative
- Topic: product quality
- Confidence: 98
- Action required: yes
</examples>

<task>
Now analyze: "The new dashboard is confusing but support was helpful"
</task>
"""

config = types.GenerateContentConfig(
    system_instruction=system_inst
)

response = client.models.generate_content(
    model="gemini-3-flash-preview",
    contents=prompt,
    config=config
)

print(response.text)
```

## Real-World Applications

- **Customer support:** Analyze feedback, prioritize responses
- **Code review:** Consistent, actionable feedback on PRs
- **Content moderation:** Classify content by policy compliance
- **Data extraction:** Structure unstructured text consistently
- **Document summarization:** Format summaries uniformly
  </action>
  <verify>
Verify both files exist and contain required sections:

```bash
cat modules/04-context-engineering/exercise.md | grep -E "(Option A:|Step [0-9]:|Success Criteria:)" | wc -l
```
Expected: At least 6 matches

```bash
cat modules/04-context-engineering/solutions/solution.md | grep -E "(Version [0-9]:|Improvement:|Key Learnings|API Code)" | wc -l
```
Expected: At least 8 matches

Verify solution shows progression:
```bash
cat modules/04-context-engineering/solutions/solution.md | grep "Version" | wc -l
```
Expected: At least 5 (showing evolution from unstructured to production-ready)
  </verify>
  <done>
Exercise file exists with guided challenge refactoring customer feedback analysis prompt through 4 versions (unstructured → system instructions → structured tags → few-shot examples), independent option for custom use cases, and Going Further section. Solution shows complete progression with explanations of improvements at each step, API code, and real-world applications.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Verify directory structure:
   ```bash
   ls -R modules/04-context-engineering/
   ```
   Expected: README.md, demonstration.md, exercise.md, prompt-templates/structured-template.md, prompt-templates/few-shot-examples.md, solutions/solution.md

2. Verify templates are reusable:
   ```bash
   grep -r "Template" modules/04-context-engineering/prompt-templates/ | wc -l
   ```
   Expected: At least 3 (template sections)

3. Verify progression is demonstrated:
   ```bash
   cat modules/04-context-engineering/solutions/solution.md | grep "Version [1-5]" | wc -l
   ```
   Expected: At least 5 (showing evolution)

4. Verify integration with previous modules:
   ```bash
   grep -ri "module 02\|structured output" modules/04-context-engineering/ | wc -l
   ```
   Expected: At least 3 mentions (showing how context engineering combines with structured output)
</verification>

<success_criteria>
Module 04 is complete when:
- [ ] All module files exist with proper structure
- [ ] Two reusable prompt templates exist (structured and few-shot patterns)
- [ ] Demonstration shows system instructions and few-shot workflow in AI Studio
- [ ] Exercise includes refactoring challenge with 4 progressive versions
- [ ] Solutions show complete progression from unstructured to production-ready
- [ ] Integration with Module 02 (structured output) is demonstrated
- [ ] Role + Task + Context + Format pattern is clearly explained
- [ ] API code shows how to implement in production
- [ ] Duration fits 20-minute target
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-04-SUMMARY.md`
</output>
