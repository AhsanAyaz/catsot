---
phase: 02-project-paths
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - part2/custom-project/template/index.html
  - part2/custom-project/template/src/main.js
  - part2/custom-project/template/src/camera.js
  - part2/custom-project/template/src/firebase.js
  - part2/custom-project/template/src/rendering.js
  - part2/custom-project/template/README.md
  - part2/custom-project/ARCHITECTURE-GUIDE.md
  - part2/custom-project/EXAMPLES.md
  - part2/custom-project/firebase.json
autonomous: true

must_haves:
  truths:
    - "Template provides working infrastructure (camera, Firebase, rendering) without imposing specific game/experience"
    - "Architecture guide helps attendees make key technical decisions"
    - "Example ideas inspire custom projects while remaining open-ended"
    - "Advanced attendees can build original projects within 60-75 minutes"
    - "Template is truly blank canvas with helpers, not another prescriptive path"
  artifacts:
    - path: "part2/custom-project/template/index.html"
      provides: "Minimal HTML5 boilerplate with containers"
      min_lines: 30
    - path: "part2/custom-project/template/src/main.js"
      provides: "Empty app initialization with helper imports"
      min_lines: 20
    - path: "part2/custom-project/template/src/camera.js"
      provides: "Optional camera setup helper (getUserMedia wrapper)"
      min_lines: 40
    - path: "part2/custom-project/template/src/firebase.js"
      provides: "Optional Firebase connection helper"
      min_lines: 50
    - path: "part2/custom-project/template/src/rendering.js"
      provides: "Optional Canvas/Three.js rendering setup"
      min_lines: 60
    - path: "part2/custom-project/ARCHITECTURE-GUIDE.md"
      provides: "Decision guide for key architectural choices"
      min_lines: 150
      contains: "Decision Matrix"
    - path: "part2/custom-project/EXAMPLES.md"
      provides: "10+ project ideas with implementation hints"
      min_lines: 200
  key_links:
    - from: "part2/custom-project/template/src/main.js"
      to: "Optional helper modules (camera, firebase, rendering)"
      via: "Import statements with comments"
      pattern: "import.*optional"
    - from: "part2/custom-project/ARCHITECTURE-GUIDE.md"
      to: "template helper modules"
      via: "References to camera.js, firebase.js, rendering.js"
      pattern: "camera\\.js.*firebase\\.js.*rendering\\.js"
---

<objective>
Create custom project option (PATH-03) with blank canvas template, architecture decision guide, and example ideas.

Purpose: Enable advanced attendees to build original projects in 60-75 minutes by providing working infrastructure helpers without imposing specific implementation, plus architectural guidance for key technical decisions.

Output: Truly open-ended project option with infrastructure helpers (template/), decision guide (ARCHITECTURE-GUIDE.md), and inspiration examples (EXAMPLES.md).
</objective>

<execution_context>
@/home/ahsan/.claude/get-shit-done/workflows/execute-plan.md
@/home/ahsan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-project-paths/02-RESEARCH.md
@.planning/REQUIREMENTS.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create blank canvas template with optional helpers</name>
  <files>
    part2/custom-project/template/index.html
    part2/custom-project/template/src/main.js
    part2/custom-project/template/src/camera.js
    part2/custom-project/template/src/firebase.js
    part2/custom-project/template/src/rendering.js
    part2/custom-project/template/README.md
    part2/custom-project/firebase.json
  </files>
  <action>
Create minimal template with optional infrastructure helpers, not prescriptive implementation.

**index.html (30 lines):**
- Minimal HTML5 boilerplate
- Single main container div (id="app")
- No pre-defined UI elements (let attendees build their own)
- CDN scripts commented out with options:
  ```html
  <!-- Optional: MediaPipe for face detection -->
  <!-- <script src="https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/vision_bundle.js"></script> -->

  <!-- Optional: Firebase for multiplayer -->
  <!-- <script src="https://www.gstatic.com/firebasejs/10.7.1/firebase-app.js"></script> -->
  <!-- <script src="https://www.gstatic.com/firebasejs/10.7.1/firebase-database.js"></script> -->

  <!-- Optional: Three.js for 3D graphics -->
  <!-- <script src="https://cdn.jsdelivr.net/npm/three@0.159.0/build/three.min.js"></script> -->

  <!-- Optional: html5-qrcode for QR scanning -->
  <!-- <script src="https://unpkg.com/html5-qrcode/minified/html5-qrcode.min.js"></script> -->
  ```
- ES6 module script loading src/main.js
- Minimal CSS: body margin 0, #app full viewport

**src/main.js (25 lines):**
```javascript
/**
 * Custom Project Template
 *
 * This is YOUR project. Build anything you want!
 *
 * Optional helpers available:
 * - camera.js: getUserMedia wrapper for camera access
 * - firebase.js: Firebase Realtime Database connection
 * - rendering.js: Canvas 2D or Three.js setup
 *
 * Uncomment imports you need. Delete what you don't.
 */

// import { setupCamera } from './camera.js';
// import { initFirebase, joinSession } from './firebase.js';
// import { setupCanvas, setupThreeJS } from './rendering.js';

async function init() {
  console.log('Your project starts here!');

  // TODO: Your initialization code
}

// Start app when DOM ready
if (document.readyState === 'loading') {
  document.addEventListener('DOMContentLoaded', init);
} else {
  init();
}
```

**src/camera.js (45 lines):**
Complete helper, not TODO. Export setupCamera() function:
```javascript
/**
 * Camera Helper
 *
 * Simple wrapper around getUserMedia for camera access.
 * Returns video element ready for use.
 */

export async function setupCamera(options = {}) {
  const {
    facingMode = 'user',  // 'user' for front camera, 'environment' for back
    width = 640,
    height = 480
  } = options;

  const constraints = {
    video: {
      facingMode,
      width: { ideal: width },
      height: { ideal: height }
    }
  };

  try {
    const stream = await navigator.mediaDevices.getUserMedia(constraints);
    const video = document.createElement('video');
    video.srcObject = stream;
    video.autoplay = true;
    video.playsInline = true;

    return new Promise((resolve, reject) => {
      video.onloadedmetadata = () => resolve(video);
      video.onerror = reject;
    });
  } catch (err) {
    console.error('Camera access denied:', err);
    throw new Error('Unable to access camera. Please grant permission.');
  }
}

export function stopCamera(video) {
  if (video && video.srcObject) {
    video.srcObject.getTracks().forEach(track => track.stop());
  }
}
```

**src/firebase.js (55 lines):**
Complete helper with emulator config:
```javascript
/**
 * Firebase Helper
 *
 * Connection to Firebase Realtime Database (using local emulator).
 * Provides basic functions for multiplayer state sync.
 */

// Emulator configuration (works locally without production Firebase)
const firebaseConfig = {
  databaseURL: "http://127.0.0.1:9000?ns=workshop-demo"
};

let db = null;

export async function initFirebase() {
  // Import Firebase modules (assumes CDN scripts loaded in HTML)
  const { initializeApp } = await import('https://www.gstatic.com/firebasejs/10.7.1/firebase-app.js');
  const { getDatabase } = await import('https://www.gstatic.com/firebasejs/10.7.1/firebase-database.js');

  const app = initializeApp(firebaseConfig);
  db = getDatabase(app);

  return db;
}

export async function writeData(path, data) {
  const { ref, set } = await import('https://www.gstatic.com/firebasejs/10.7.1/firebase-database.js');
  const dataRef = ref(db, path);
  await set(dataRef, data);
}

export async function listenToData(path, callback) {
  const { ref, onValue } = await import('https://www.gstatic.com/firebasejs/10.7.1/firebase-database.js');
  const dataRef = ref(db, path);
  onValue(dataRef, snapshot => {
    callback(snapshot.val());
  });
}

export async function setupPresence(path, onlineData, offlineData) {
  const { ref, onDisconnect, set } = await import('https://www.gstatic.com/firebasejs/10.7.1/firebase-database.js');
  const presenceRef = ref(db, path);

  // Set online status
  await set(presenceRef, onlineData);

  // Set offline status when disconnected
  onDisconnect(presenceRef).set(offlineData);
}
```

**src/rendering.js (70 lines):**
Complete helper with Canvas 2D and Three.js options:
```javascript
/**
 * Rendering Helper
 *
 * Setup for Canvas 2D or Three.js rendering.
 * Choose one based on your project needs.
 */

/**
 * Setup Canvas 2D rendering
 * Returns { canvas, ctx, width, height }
 */
export function setupCanvas(containerId = 'app') {
  const container = document.getElementById(containerId);

  const canvas = document.createElement('canvas');
  canvas.width = window.innerWidth;
  canvas.height = window.innerHeight;
  container.appendChild(canvas);

  const ctx = canvas.getContext('2d');

  // Handle window resize
  window.addEventListener('resize', () => {
    canvas.width = window.innerWidth;
    canvas.height = window.innerHeight;
  });

  return { canvas, ctx, width: canvas.width, height: canvas.height };
}

/**
 * Setup Three.js rendering
 * Returns { scene, camera, renderer }
 */
export function setupThreeJS(containerId = 'app') {
  // Assumes Three.js loaded from CDN
  if (typeof THREE === 'undefined') {
    throw new Error('Three.js not loaded. Uncomment CDN script in index.html');
  }

  const container = document.getElementById(containerId);

  // Scene
  const scene = new THREE.Scene();

  // Camera
  const camera = new THREE.PerspectiveCamera(
    75,
    window.innerWidth / window.innerHeight,
    0.1,
    1000
  );
  camera.position.z = 5;

  // Renderer
  const renderer = new THREE.WebGLRenderer({ antialias: true });
  renderer.setSize(window.innerWidth, window.innerHeight);
  renderer.setPixelRatio(Math.min(window.devicePixelRatio, 2));
  container.appendChild(renderer.domElement);

  // Handle window resize
  window.addEventListener('resize', () => {
    camera.aspect = window.innerWidth / window.innerHeight;
    camera.updateProjectionMatrix();
    renderer.setSize(window.innerWidth, window.innerHeight);
  });

  return { scene, camera, renderer };
}
```

**README.md:**
```markdown
# Custom Project Template

This is a blank canvas for building your own AI-powered interactive experience.

## What's Included

**Infrastructure helpers (optional):**
- `src/camera.js` - Camera access via getUserMedia
- `src/firebase.js` - Firebase Realtime Database connection (emulator)
- `src/rendering.js` - Canvas 2D or Three.js setup

**You decide:**
- What to build
- Which helpers to use (or none)
- How to structure your code

## Getting Started

1. **Choose your approach** (see ARCHITECTURE-GUIDE.md)
   - Solo or multiplayer?
   - 2D or 3D?
   - Camera input or other interaction?

2. **Uncomment what you need** in index.html and main.js
   - MediaPipe for face detection
   - Firebase for multiplayer
   - Three.js for 3D
   - html5-qrcode for QR scanning

3. **Start building** in src/main.js

4. **Get inspired** (see EXAMPLES.md for project ideas)

## Need Ideas?

See EXAMPLES.md for 10+ project concepts with implementation hints.

## Architecture Questions?

See ARCHITECTURE-GUIDE.md for decision matrix on:
- Canvas 2D vs Three.js
- Firebase Realtime DB vs local state
- Face detection vs QR codes vs other inputs

## Time Budget

You have 60-75 minutes. Focus on ONE core feature:
- ✅ Working camera + simple visual effect
- ❌ Complex game + multiplayer + 3D graphics + AI
```

**firebase.json:**
Same emulator config as camera-game project.
  </action>
  <verify>
Check template files exist:
- `ls part2/custom-project/template/index.html`
- `ls part2/custom-project/template/src/*.js` shows 4 files
- `grep -l "Optional:" part2/custom-project/template/index.html`
- `grep -l "TODO: Your initialization" part2/custom-project/template/src/main.js`
- `grep -c "export" part2/custom-project/template/src/camera.js` > 1
- `grep -c "export" part2/custom-project/template/src/firebase.js` > 2
- `grep -c "export" part2/custom-project/template/src/rendering.js` > 1
  </verify>
  <done>
Blank canvas template exists with optional infrastructure helpers (camera, Firebase, rendering) ready to use but not prescriptive. Attendees can import what they need and delete the rest.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create architecture decision guide</name>
  <files>
    part2/custom-project/ARCHITECTURE-GUIDE.md
  </files>
  <action>
Create comprehensive decision guide for key architectural choices attendees will face.

**Structure:**

# Architecture Decision Guide

## Introduction

Building a custom project means making architectural decisions. This guide helps you choose the right tools and patterns for YOUR idea.

**Time constraint:** You have 60-75 minutes. Make decisions quickly, execute confidently.

## Decision Matrix

Use this table to make quick architectural decisions:

| Your Project Needs... | Choose | Why | Helper Module |
|----------------------|--------|-----|---------------|
| 2D particles, shapes, drawings | Canvas 2D | Simpler API, better for 2D | `rendering.js` setupCanvas() |
| 3D objects, lighting, perspective | Three.js | WebGL power, 3D scene management | `rendering.js` setupThreeJS() |
| Face tracking, expressions | MediaPipe | 478 landmarks + 52 blendshapes | Research Pattern 1 |
| QR code scanning | html5-qrcode | Cross-platform camera handling | Research Pattern 4 |
| Multiplayer sync | Firebase Realtime DB | WebSocket-based live updates | `firebase.js` + emulator |
| Solo experience | Local state | No network complexity | Just JavaScript variables |
| Camera input | getUserMedia | Standard browser API | `camera.js` setupCamera() |
| Keyboard/mouse only | Event listeners | No camera needed | Standard DOM events |

## Key Decisions

### 1. Canvas 2D vs Three.js

**Choose Canvas 2D if:**
- Building 2D particle systems
- Drawing shapes, text, images
- Simple animations
- Comfort with immediate-mode rendering
- Example: Emotion-driven particle visualization

**Choose Three.js if:**
- Building 3D scenes
- Need lighting, shadows, materials
- Want perspective camera
- Comfort with scene graph
- Example: 3D floating objects controlled by gestures

**Time impact:**
- Canvas 2D: ~5 min setup, focus on logic
- Three.js: ~15 min setup (scene, camera, renderer), more complex logic

**Code comparison:**

Canvas 2D:
```javascript
const { canvas, ctx } = setupCanvas();
ctx.fillStyle = 'blue';
ctx.fillRect(100, 100, 50, 50);
```

Three.js:
```javascript
const { scene, camera, renderer } = setupThreeJS();
const geometry = new THREE.BoxGeometry(1, 1, 1);
const material = new THREE.MeshBasicMaterial({ color: 0x0000ff });
const cube = new THREE.Mesh(geometry, material);
scene.add(cube);
```

### 2. Firebase vs Local State

**Choose Firebase if:**
- Multiple players interacting
- Need real-time sync across devices
- Want presence detection (who's online)
- Example: Collaborative drawing, multiplayer game

**Choose Local State if:**
- Solo experience
- No need for persistence
- Avoiding network complexity
- Example: Personal emotion visualizer

**Time impact:**
- Firebase: ~10 min setup (emulator, connection, listeners)
- Local state: ~2 min setup (just variables)

**Code comparison:**

Firebase:
```javascript
const db = await initFirebase();
await writeData('users/abc/score', 100);
listenToData('users/abc/score', (score) => {
  console.log('Score updated:', score);
});
```

Local state:
```javascript
let score = 100;
function updateScore(newScore) {
  score = newScore;
  console.log('Score updated:', score);
}
```

### 3. MediaPipe vs QR Codes vs Other Input

**Choose MediaPipe if:**
- Facial expressions drive interaction
- Hand gestures control elements
- Pose detection needed
- Example: Face-reactive visuals, gesture-controlled camera

**Choose QR Codes if:**
- Session creation/joining
- Physical object interaction
- Multiplayer matchmaking
- Example: Scan to join game, scan objects for effects

**Choose Other Input if:**
- Keyboard/mouse sufficient
- Voice input (Web Speech API)
- Device motion (accelerometer)
- Example: Typing game, voice-controlled, tilt-to-move

**Time impact:**
- MediaPipe: ~20 min (initialization, processing loop, mapping)
- QR Codes: ~10 min (scanner setup, session handling)
- Keyboard/mouse: ~5 min (event listeners)

## Decision Workflow

Follow this workflow to make decisions quickly:

```
START
  ↓
[Do you need multiplayer?]
  ├─ YES → Use Firebase (firebase.js)
  └─ NO → Use local state
  ↓
[Do you need camera input?]
  ├─ YES → Do you need face/hand detection?
  │         ├─ YES → Use MediaPipe
  │         └─ NO → QR codes? (html5-qrcode) or just video? (camera.js)
  └─ NO → Use keyboard/mouse events
  ↓
[Do you need 3D graphics?]
  ├─ YES → Use Three.js (rendering.js setupThreeJS)
  └─ NO → Use Canvas 2D (rendering.js setupCanvas)
  ↓
BUILD!
```

## Common Patterns

### Pattern: Face-Reactive Solo Experience
- **Stack:** MediaPipe + Canvas 2D
- **Time:** ~45 min implementation
- **Structure:** Face detection → emotion mapping → visual update
- **Example:** Particles change color/speed based on expressions

### Pattern: Multiplayer QR Game
- **Stack:** html5-qrcode + Firebase + Canvas 2D
- **Time:** ~50 min implementation
- **Structure:** QR scan → join session → sync state → render game
- **Example:** Speed clicker with live scoreboard

### Pattern: 3D Gesture Control
- **Stack:** MediaPipe Hand Landmarker + Three.js
- **Time:** ~60 min implementation
- **Structure:** Hand tracking → gesture recognition → 3D object manipulation
- **Example:** Rotate/scale 3D object with hand gestures

### Pattern: Voice-Controlled Visualization
- **Stack:** Web Speech API + Canvas 2D
- **Time:** ~40 min implementation
- **Structure:** Speech recognition → command parsing → visual effect
- **Example:** Say emotions, see visual changes

## Anti-Patterns to Avoid

### ❌ Using Everything
Don't import all helpers "just in case". Choose what you NEED.

**Bad:**
```javascript
import { setupCamera } from './camera.js';
import { initFirebase } from './firebase.js';
import { setupCanvas, setupThreeJS } from './rendering.js';
// Using none of these effectively
```

**Good:**
```javascript
import { setupCanvas } from './rendering.js';
// Focused on Canvas 2D visualization
```

### ❌ Overscoping
Don't try to build multiplayer + 3D + AI in 75 minutes.

**Bad:** "I'll make a multiplayer 3D game where AI generates levels based on facial expressions"

**Good:** "I'll make a 3D object that rotates based on detected facial expressions"

### ❌ Premature Optimization
Don't spend 30 minutes on object pooling before you have working visuals.

**Bad:** "Let me optimize particle allocation first..."

**Good:** "Let me get particles rendering first, optimize if needed"

## Time Budget Guidance

Break your 60-75 minutes like this:

- **0-10 min:** Decision making (use this guide)
- **10-20 min:** Infrastructure setup (camera, rendering, Firebase)
- **20-50 min:** Core feature implementation
- **50-60 min:** Testing and polish
- **60-75 min:** Extensions or bug fixes

**Checkpoint at 30 min:** Do you have SOMETHING working? (even if basic)
- ✅ YES → Continue building
- ❌ NO → Simplify scope NOW

## Getting Unstuck

**Stuck on MediaPipe?**
- Reference: 02-RESEARCH.md Pattern 1 (MediaPipe Face Landmarker)
- Fallback: Use simpler input (keyboard, mouse)

**Stuck on Firebase?**
- Check emulator is running: `firebase emulators:start --only database`
- Reference: 02-RESEARCH.md Pattern 2 (Firebase multiplayer)
- Fallback: Use local state, add Firebase later

**Stuck on Three.js?**
- Reference: 02-RESEARCH.md Pattern 3 (Canvas 2D) for simpler alternative
- Fallback: Switch to Canvas 2D, get it working, upgrade to Three.js if time

**Stuck on concept?**
- See EXAMPLES.md for project ideas
- Simplify: Pick ONE feature, make it work well

## Success Criteria

Your custom project is successful if:
- ✅ It works (no errors in console)
- ✅ It demonstrates ONE core feature well
- ✅ You can explain what it does
- ✅ You learned something new

NOT required:
- ❌ Perfect visuals
- ❌ Production-ready code
- ❌ All features you imagined
- ❌ Impressing others

**Remember:** Working prototype > ambitious failure.
  </action>
  <verify>
Check ARCHITECTURE-GUIDE.md exists and contains decision matrix:
- `grep -l "Decision Matrix" part2/custom-project/ARCHITECTURE-GUIDE.md`
- `grep -c "Choose.*if:" part2/custom-project/ARCHITECTURE-GUIDE.md` > 5
- `grep -l "Anti-Patterns" part2/custom-project/ARCHITECTURE-GUIDE.md`
- `wc -l part2/custom-project/ARCHITECTURE-GUIDE.md` > 150
  </verify>
  <done>
Architecture decision guide exists with decision matrix, workflow diagram, common patterns, anti-patterns, time budget guidance, and getting unstuck strategies. Attendees can make quick architectural decisions confidently.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create example project ideas document</name>
  <files>
    part2/custom-project/EXAMPLES.md
  </files>
  <action>
Create inspiring project examples with implementation hints but remaining open-ended.

**Structure:**

# Custom Project Examples

## Introduction

Need inspiration? Here are 15+ project ideas you can build in 60-75 minutes.

Each example includes:
- **Concept:** What it does
- **Stack:** Which tools to use
- **Core Feature:** The ONE thing to implement
- **Implementation Hints:** How to approach it
- **Extension Ideas:** Where to go next

Pick one, adapt it, or combine ideas to create something unique!

## Solo Experiences

### 1. Emotion Mirror
**Concept:** Your facial expression changes the visual atmosphere around you.

**Stack:** MediaPipe + Canvas 2D

**Core Feature:** Map facial blendshapes to visual parameters (color, particle density, movement speed).

**Implementation Hints:**
1. Setup MediaPipe Face Landmarker (02-RESEARCH.md Pattern 1)
2. Extract key blendshapes (smile, frown, eyebrow raises)
3. Map to color palette (happy = warm, sad = cool)
4. Update Canvas particle system based on emotion
5. Use object pooling for performance

**Extension Ideas:**
- Add emotion history graph
- Smooth emotion transitions
- Multiple simultaneous emotions

**Time Estimate:** 45-55 min

---

### 2. Voice Painter
**Concept:** Speak words, see them visualized as abstract art.

**Stack:** Web Speech API + Canvas 2D

**Core Feature:** Convert speech to text, analyze sentiment/words, generate visual representation.

**Implementation Hints:**
1. Setup Web Speech API (SpeechRecognition)
2. Capture transcript in real-time
3. Analyze word sentiment (positive/negative/neutral)
4. Draw shapes/colors based on sentiment
5. Build up visual composition over time

**Extension Ideas:**
- Word cloud visualization
- Save as image
- Background music based on sentiment

**Time Estimate:** 40-50 min

---

### 3. Gesture Conductor
**Concept:** Control 3D objects with hand gestures like a conductor.

**Stack:** MediaPipe Hand Landmarker + Three.js

**Core Feature:** Track hand position, map to 3D object rotation/scale/position.

**Implementation Hints:**
1. Setup MediaPipe Hand Landmarker
2. Track index finger tip position (landmark 8)
3. Map x/y position to 3D rotation
4. Add pinch gesture for scale (distance between thumb and index)
5. Render simple 3D cube or sphere

**Extension Ideas:**
- Multiple objects
- Gesture vocabulary (swipe, circle, etc.)
- Particle trail following hand

**Time Estimate:** 55-65 min

---

### 4. Mood Soundscape
**Concept:** Your expression changes ambient sound layers.

**Stack:** MediaPipe + Web Audio API

**Core Feature:** Detect emotion, adjust audio parameters (volume, filter, reverb) per emotion.

**Implementation Hints:**
1. Setup MediaPipe Face Landmarker
2. Load 3-4 ambient audio loops (rain, birds, wind, music)
3. Detect emotion (happy, sad, calm, excited)
4. Adjust volume/filters per track based on emotion
5. Smooth transitions between states

**Extension Ideas:**
- Record audio output
- Visualize audio waveforms
- User-uploaded audio files

**Time Estimate:** 50-60 min

---

### 5. Focus Timer
**Concept:** Track if you're looking at screen, penalize distractions.

**Stack:** MediaPipe Face Landmarker + Canvas 2D

**Core Feature:** Detect face presence, measure time looking at screen, pause timer if face disappears.

**Implementation Hints:**
1. Setup MediaPipe Face Landmarker
2. Track face detection confidence
3. If confidence > 0.5: increment focus time
4. If confidence < 0.5: pause, show "Come back!" message
5. Display total focus time and breaks

**Extension Ideas:**
- Pomodoro integration
- Focus streak tracking
- Export focus report

**Time Estimate:** 35-45 min

---

## Multiplayer Experiences

### 6. Emoji Reaction Wall
**Concept:** Players send emoji reactions that appear on shared wall in real-time.

**Stack:** Firebase + Canvas 2D

**Core Feature:** Click emoji, broadcast to all players, animate on shared canvas.

**Implementation Hints:**
1. Setup Firebase Realtime Database emulator
2. Write emoji + position to `/reactions/{timestamp}`
3. Listen to new reactions
4. Animate emoji floating up screen (like Twitch reactions)
5. Auto-delete old reactions after 5 seconds

**Extension Ideas:**
- Emoji rain effect
- Reaction counts
- Custom emoji uploads

**Time Estimate:** 40-50 min

---

### 7. Collaborative Drawing
**Concept:** Multiple people draw on same canvas in real-time.

**Stack:** Firebase + Canvas 2D

**Core Feature:** Mouse/touch draws lines, sync stroke data to Firebase, render all players' strokes.

**Implementation Hints:**
1. Setup Firebase Realtime Database
2. Track mouse/touch position
3. Write stroke data: `{ playerId, x, y, color, timestamp }`
4. Listen to all strokes, render to canvas
5. Color-code by player

**Extension Ideas:**
- Undo/redo
- Clear canvas
- Save as image

**Time Estimate:** 45-55 min

---

### 8. Presence Heatmap
**Concept:** Show where players are looking on screen (mouse position heatmap).

**Stack:** Firebase + Canvas 2D

**Core Feature:** Track mouse position, broadcast to Firebase, render density heatmap.

**Implementation Hints:**
1. Setup Firebase Realtime Database
2. Write mouse position every 100ms: `/positions/{playerId}`
3. Listen to all player positions
4. Render heatmap using gradient circles
5. Fade old positions over time

**Extension Ideas:**
- Click heatmap (where people click)
- Path visualization (movement trails)
- Attention analytics

**Time Estimate:** 50-60 min

---

### 9. Turn-Based Photo Game
**Concept:** Players take turns capturing photos, others vote on best one.

**Stack:** html5-qrcode + Firebase + Camera API

**Core Feature:** Capture photo, upload to Firebase, display grid, voting system.

**Implementation Hints:**
1. Setup camera with getUserMedia
2. Capture still frame to canvas
3. Convert to data URL
4. Write to Firebase: `/photos/{photoId}`
5. Simple voting: write to `/votes/{photoId}/{playerId}`

**Extension Ideas:**
- Photo challenges (themes)
- Filters/effects
- Winner announcement

**Time Estimate:** 55-65 min

---

## Creative Experiments

### 10. Word-Driven Fractals
**Concept:** Type words, see fractal patterns based on word properties.

**Stack:** Canvas 2D + Web APIs

**Core Feature:** Analyze word (length, vowels, consonants), map to fractal parameters.

**Implementation Hints:**
1. Setup text input
2. Analyze word: length, vowel ratio, letter frequencies
3. Map to fractal recursion depth, angle, color
4. Draw recursive tree fractal
5. Animate parameter transitions

**Extension Ideas:**
- Multiple fractal types
- Save fractal as image
- Word history visualization

**Time Estimate:** 45-55 min

---

### 11. Beat-Driven Particles
**Concept:** Microphone detects audio beats, triggers particle bursts.

**Stack:** Web Audio API + Canvas 2D

**Core Feature:** Analyze microphone input for beat detection, emit particles on beats.

**Implementation Hints:**
1. Setup getUserMedia for audio
2. Use AnalyserNode for frequency data
3. Detect beats (sudden energy increase in low frequencies)
4. Emit particle burst on beat
5. Vary particle properties by beat intensity

**Extension Ideas:**
- Frequency-based colors
- Waveform visualization
- Record video output

**Time Estimate:** 50-60 min

---

### 12. Gravity Sandbox
**Concept:** Click to spawn objects with physics, interact with gravity.

**Stack:** Canvas 2D + Basic Physics

**Core Feature:** Spawn circles on click, apply gravity, bounce off edges.

**Implementation Hints:**
1. Setup Canvas 2D
2. Store objects: `{ x, y, vx, vy, radius, color }`
3. Update loop: apply gravity (vy += 0.5), update position
4. Check edge collisions, reverse velocity
5. Render circles

**Extension Ideas:**
- Object-to-object collisions
- Drag to throw objects
- Attractors/repellers

**Time Estimate:** 40-50 min

---

### 13. ASCII Camera
**Concept:** Convert camera feed to ASCII art in real-time.

**Stack:** Camera API + Canvas 2D

**Core Feature:** Sample video pixels, map brightness to ASCII characters, render as text.

**Implementation Hints:**
1. Setup camera with getUserMedia
2. Sample video frame pixels in grid (e.g., every 8x8)
3. Calculate average brightness per cell
4. Map brightness to ASCII: ` .:-=+*#%@`
5. Render ASCII to canvas or DOM

**Extension Ideas:**
- Color ASCII
- Adjustable resolution
- Save as text file

**Time Estimate:** 45-55 min

---

### 14. Particle Attraction
**Concept:** Mouse/face position attracts particles creating flowing patterns.

**Stack:** MediaPipe or Mouse + Canvas 2D

**Core Feature:** Particles attracted to target position with spring physics.

**Implementation Hints:**
1. Setup particle system (object pooling)
2. Get target position (mouse or face landmark)
3. Calculate force toward target: `force = (target - position) * strength`
4. Update velocity: `velocity += force`
5. Update position: `position += velocity * damping`

**Extension Ideas:**
- Multiple attractors
- Repulsion zones
- Particle connections

**Time Estimate:** 40-50 min

---

### 15. Expression Recorder
**Concept:** Record sequence of facial expressions, play back as animation.

**Stack:** MediaPipe + Canvas 2D

**Core Feature:** Capture blendshape values over time, replay as visualization.

**Implementation Hints:**
1. Setup MediaPipe Face Landmarker
2. Record blendshapes to array: `{ timestamp, blendshapes }`
3. Add record/stop buttons
4. Playback mode: interpolate between recorded frames
5. Visualize key blendshapes as bars

**Extension Ideas:**
- Export as JSON
- Compare recordings
- Emotion timeline

**Time Estimate:** 50-60 min

---

## Mix and Match

Combine elements from different examples:

- **Emotion Mirror** + **Multiplayer** = Shared emotion visualization
- **Gesture Conductor** + **Beat-Driven Particles** = Gesture-controlled music visualizer
- **Collaborative Drawing** + **Voice Painter** = Voice-controlled shared canvas
- **Focus Timer** + **Presence Heatmap** = Productivity monitoring for teams

## Still Stuck?

**Ask yourself:**
1. What INPUT do you want? (face, voice, mouse, QR, audio)
2. What OUTPUT do you want? (visuals, audio, data)
3. What's the CONNECTION? (emotion → color, gesture → rotation, beat → burst)

**Start simple:**
- Get INPUT working (detect face, capture voice, track mouse)
- Get OUTPUT working (draw particles, play sound, show text)
- Connect them (input changes output)
- Polish and extend

**Remember:** These are starting points. Make them YOUR OWN!
  </action>
  <verify>
Check EXAMPLES.md exists and contains project ideas:
- `grep -c "Concept:" part2/custom-project/EXAMPLES.md` >= 15
- `grep -c "Implementation Hints:" part2/custom-project/EXAMPLES.md` >= 15
- `grep -l "Solo Experiences.*Multiplayer.*Creative" part2/custom-project/EXAMPLES.md`
- `wc -l part2/custom-project/EXAMPLES.md` > 200
  </verify>
  <done>
Example project ideas document exists with 15 detailed examples across solo, multiplayer, and creative categories. Each example includes concept, stack, core feature, implementation hints, extensions, and time estimate. Attendees have clear inspiration without being prescriptive.
  </done>
</task>

</tasks>

<verification>
Custom project option complete when:
- [ ] Template exists with minimal structure and optional helpers (camera, Firebase, rendering)
- [ ] Template helpers are complete and working (no TODOs, ready to use)
- [ ] ARCHITECTURE-GUIDE.md exists with decision matrix and workflow
- [ ] EXAMPLES.md exists with 15+ project ideas with implementation hints
- [ ] Template is truly blank canvas (no prescriptive implementation)
- [ ] Firebase emulator configuration is ready (firebase.json)
- [ ] All helper modules are independently usable (can import one without others)
</verification>

<success_criteria>
PATH-03 requirement met when:
1. Opening template/index.html shows blank page with single container div (no UI imposed)
2. Template main.js has commented import statements for optional helpers
3. Uncommenting camera.js import and calling setupCamera() returns working video element
4. Uncommenting firebase.js import and calling initFirebase() connects to emulator
5. Uncommenting rendering.js import and calling setupCanvas() returns working canvas context
6. ARCHITECTURE-GUIDE.md provides clear decision workflow with time impact estimates
7. EXAMPLES.md provides at least 10 project ideas spanning different input/output combinations
8. Advanced attendees can build custom project in 60-75 minutes using provided infrastructure
9. Template does NOT look like prescriptive path (unlike 02-01 and 02-02 which have specific TODOs)
</success_criteria>

<output>
After completion, create `.planning/phases/02-project-paths/02-03-SUMMARY.md`
</output>
