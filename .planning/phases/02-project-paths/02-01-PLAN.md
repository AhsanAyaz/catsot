---
phase: 02-project-paths
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - part2/face-reactive/starter/index.html
  - part2/face-reactive/starter/src/main.js
  - part2/face-reactive/starter/src/faceDetection.js
  - part2/face-reactive/starter/src/emotionMapping.js
  - part2/face-reactive/starter/src/visualization.js
  - part2/face-reactive/starter/README.md
  - part2/face-reactive/reference/index.html
  - part2/face-reactive/reference/src/main.js
  - part2/face-reactive/reference/src/faceDetection.js
  - part2/face-reactive/reference/src/emotionMapping.js
  - part2/face-reactive/reference/src/visualization.js
  - part2/face-reactive/EXTENSIONS.md
autonomous: true

must_haves:
  truths:
    - "Starter template provides 60-70% complete scaffolding with strategic TODOs"
    - "Reference implementation demonstrates working face-reactive experience"
    - "MediaPipe Face Landmarker detects facial expressions and extracts blendshapes"
    - "Blendshapes map to simplified emotions (happy, sad, surprised, angry, neutral, calm)"
    - "Emotion-driven particle visualization updates based on detected expressions"
    - "Extension challenges guide attendees beyond basic implementation"
  artifacts:
    - path: "part2/face-reactive/starter/index.html"
      provides: "HTML boilerplate with video element and canvas"
      min_lines: 40
    - path: "part2/face-reactive/starter/src/main.js"
      provides: "App initialization with camera setup and render loop (COMPLETE)"
      min_lines: 50
    - path: "part2/face-reactive/starter/src/faceDetection.js"
      provides: "MediaPipe Face Landmarker wrapper (COMPLETE)"
      min_lines: 60
    - path: "part2/face-reactive/starter/src/emotionMapping.js"
      provides: "Blendshape to emotion mapping (TODO for attendees)"
      min_lines: 40
      contains: "TODO"
    - path: "part2/face-reactive/starter/src/visualization.js"
      provides: "Particle system with emotion-driven visuals (TODO for attendees)"
      min_lines: 80
      contains: "TODO"
    - path: "part2/face-reactive/reference/src/emotionMapping.js"
      provides: "Complete emotion detection logic with threshold-based rules"
      min_lines: 60
    - path: "part2/face-reactive/reference/src/visualization.js"
      provides: "Complete particle system with object pooling and emotion parameters"
      min_lines: 150
  key_links:
    - from: "part2/face-reactive/starter/src/main.js"
      to: "src/faceDetection.js"
      via: "import and initialization"
      pattern: "import.*faceDetection"
    - from: "part2/face-reactive/starter/src/faceDetection.js"
      to: "src/emotionMapping.js"
      via: "blendshapes passed to emotion detector"
      pattern: "detectEmotion\\(blendshapes\\)"
    - from: "part2/face-reactive/starter/src/main.js"
      to: "src/visualization.js"
      via: "emotion updates trigger visual changes"
      pattern: "setEmotion\\(emotion\\)"
---

<objective>
Create face-reactive experience project path (PATH-01) with starter template, reference implementation, and extension challenges.

Purpose: Enable attendees to build MediaPipe-powered emotion visualization in 60-75 minutes by providing complete infrastructure (camera, face detection, rendering loop) with strategic TODOs for emotion mapping and visual effects.

Output: Complete project option with 60-70% scaffolding (starter/), 100% working example (reference/), and guided extensions for fast finishers.
</objective>

<execution_context>
@/home/ahsan/.claude/get-shit-done/workflows/execute-plan.md
@/home/ahsan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-project-paths/02-RESEARCH.md
@.planning/REQUIREMENTS.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create starter template with complete infrastructure</name>
  <files>
    part2/face-reactive/starter/index.html
    part2/face-reactive/starter/src/main.js
    part2/face-reactive/starter/src/faceDetection.js
    part2/face-reactive/starter/src/emotionMapping.js
    part2/face-reactive/starter/src/visualization.js
    part2/face-reactive/starter/README.md
  </files>
  <action>
Create 60-70% complete starter template following research patterns from 02-RESEARCH.md.

**index.html:**
- Basic HTML5 boilerplate with viewport meta
- Video element (id="webcam") for camera feed (hidden with CSS)
- Canvas element (id="canvas") for particle visualization
- Status div for camera permission feedback
- CDN script for MediaPipe tasks-vision: https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/vision_bundle.js
- ES6 module script tag loading src/main.js

**src/main.js (COMPLETE - 60 lines):**
- Camera setup function with getUserMedia and permission handling
- Render loop with requestAnimationFrame pattern
- App initialization that sets up camera, MediaPipe, and visualization
- Error handling with status messages
- Integration points calling faceDetection.js and visualization.js

**src/faceDetection.js (COMPLETE - 70 lines):**
- MediaPipe FilesetResolver initialization
- FaceLandmarker.createFromOptions with GPU delegate, VIDEO mode, outputFaceBlendshapes: true
- Video processing loop checking video.currentTime !== lastVideoTime (avoid duplicate frames)
- Export initFaceLandmarker() and processVideoFrame(video, callback) functions
- Include comment: "// COMPLETE - Face detection infrastructure ready"

**src/emotionMapping.js (TODO - 45 lines with scaffolding):**
- Helper function getBlendshapeScore(blendshapes, name) (COMPLETE)
- Export detectEmotion(blendshapes) function with TODO
- Include blendshape name constants (mouthSmileLeft, mouthSmileRight, browInnerUp, etc.)
- Comment with TODO instructions:
  ```javascript
  // TODO 1: Implement emotion detection (15 minutes)
  // HINT: Check which blendshapes are > 0.5 threshold
  // HINT: mouthSmileLeft + mouthSmileRight > 1.0 = happy
  // HINT: browInnerUp + eyeWideLeft/Right > 0.8 = surprised
  // HINT: browDownLeft + browDownRight > 0.8 = angry
  // Return one of: 'happy', 'sad', 'surprised', 'angry', 'neutral', 'calm'
  ```
- Placeholder: `return 'neutral'; // Replace with your logic`

**src/visualization.js (TODO - 90 lines with scaffolding):**
- Complete ParticleSystem class constructor with object pooling (150 particles pre-created)
- Complete update() method (position updates, edge wrapping)
- Complete render() method (canvas drawing with fade effect)
- Emotion-to-visual mapping object provided (colors, velocities, sizes, particle counts)
- Export setEmotion(emotion) function with TODO:
  ```javascript
  // TODO 2: Update particles based on emotion (20 minutes)
  // HINT: Use the emotionParams map below
  // HINT: Update existing particles (don't create new ones - object pooling)
  // HINT: Set p.color, p.size, p.vx, p.vy, p.active based on emotion params
  ```
- Placeholder: `// TODO: Implement particle updates here`

**README.md:**
- Project description: "Face-reactive emotion visualization using MediaPipe and Canvas"
- Learning objectives: MediaPipe Face Landmarker, blendshape analysis, particle systems, object pooling
- Setup: Open index.html in browser, allow camera access
- Tasks: List TODO 1 (emotion detection - 15 min) and TODO 2 (particle updates - 20 min)
- Testing: Instructions to smile (happy), raise eyebrows (surprised), frown (sad)
- Extension challenges: Reference EXTENSIONS.md
- Time estimate: 60-75 minutes including testing
  </action>
  <verify>
Check that all 6 files exist and contain expected structure:
- `grep -l "TODO 1" part2/face-reactive/starter/src/emotionMapping.js`
- `grep -l "TODO 2" part2/face-reactive/starter/src/visualization.js`
- `grep -l "COMPLETE" part2/face-reactive/starter/src/faceDetection.js`
- `grep -l "COMPLETE" part2/face-reactive/starter/src/main.js`
- `wc -l part2/face-reactive/starter/src/*.js` shows appropriate line counts
  </verify>
  <done>
Starter template exists with complete infrastructure (camera, MediaPipe, rendering loop) and 2 strategic TODOs for emotion detection and particle updates. Opening index.html shows camera permission prompt and blank canvas ready for implementation.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create reference implementation</name>
  <files>
    part2/face-reactive/reference/index.html
    part2/face-reactive/reference/src/main.js
    part2/face-reactive/reference/src/faceDetection.js
    part2/face-reactive/reference/src/emotionMapping.js
    part2/face-reactive/reference/src/visualization.js
  </files>
  <action>
Create fully working reference implementation based on research patterns from 02-RESEARCH.md.

**Copy starter infrastructure (index.html, main.js, faceDetection.js):**
- Use identical files from starter/ for consistency
- These are already complete

**src/emotionMapping.js (COMPLETE - 70 lines):**
- Implement complete detectEmotion(blendshapes) function
- Extract key blendshape values: smileLeft, smileRight, frownLeft, frownRight, browInnerUp, eyeWideLeft, eyeWideRight, jawOpen, browDownLeft, browDownRight
- Emotion detection with 0.5 threshold:
  - Happy: smileLeft > 0.5 AND smileRight > 0.5
  - Surprised: browInnerUp > 0.5 AND (eyeWideLeft > 0.5 OR eyeWideRight > 0.5)
  - Sad: (frownLeft > 0.5 OR frownRight > 0.5) AND browInnerUp < 0.2
  - Angry: browDownLeft > 0.5 AND browDownRight > 0.5
  - Excited: (smileLeft + smileRight) > 1.0 AND jawOpen > 0.4
  - Default: 'calm'
- Return object: `{ emotion: string, confidence: number }`
- Include comments explaining threshold logic

**src/visualization.js (COMPLETE - 160 lines):**
- Complete ParticleSystem class (same constructor, update, render as starter)
- Implement setEmotion(emotion) method:
  - Define emotionParams map with 6 emotions (happy, sad, excited, calm, surprised, angry)
  - Each emotion has: color (hex), velocityMultiplier, sizeMultiplier, particleCount
  - Example params from research:
    - happy: `{ color: '#FFD700', velocityMultiplier: 2.0, sizeMultiplier: 1.2, particleCount: 100 }`
    - sad: `{ color: '#4A4A8A', velocityMultiplier: 0.3, sizeMultiplier: 0.8, particleCount: 50 }`
    - surprised: `{ color: '#FFEB3B', velocityMultiplier: 4.0, sizeMultiplier: 1.0, particleCount: 120 }`
  - Update existing particles (object pooling pattern):
    ```javascript
    for (let i = 0; i < this.maxParticles; i++) {
      const p = this.particles[i];
      p.active = i < params.particleCount;
      p.color = params.color;
      p.size = 5 * params.sizeMultiplier;
      p.vx = (Math.random() - 0.5) * params.velocityMultiplier;
      p.vy = (Math.random() - 0.5) * params.velocityMultiplier;
    }
    ```
- Include performance optimization comments (object pooling, no allocation in render loop)

Use patterns from 02-RESEARCH.md sections: Pattern 1 (MediaPipe), Pattern 3 (Canvas 2D particles), Code Examples (blendshape mapping, emotion visualization).
  </action>
  <verify>
Check reference implementation is complete:
- `grep -c "TODO" part2/face-reactive/reference/src/*.js` returns 0 (no TODOs)
- `node -c part2/face-reactive/reference/src/emotionMapping.js` (syntax check)
- `node -c part2/face-reactive/reference/src/visualization.js` (syntax check)
- `grep -l "happy.*sad.*surprised.*angry.*calm" part2/face-reactive/reference/src/emotionMapping.js`
- `grep -l "object pooling" part2/face-reactive/reference/src/visualization.js`
  </verify>
  <done>
Reference implementation exists with complete emotion detection (6 emotions with thresholds) and particle system (object pooling, emotion-driven parameters). Opening index.html shows working face-reactive visualization that changes colors and movement based on facial expressions.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create extension challenges document</name>
  <files>
    part2/face-reactive/EXTENSIONS.md
  </files>
  <action>
Create extension challenges for fast finishers, organized by difficulty and estimated time.

**Structure:**

# Face-Reactive Experience - Extension Challenges

## Overview
Completed the basic emotion visualization? Here are ways to extend your project.

## Beginner Extensions (10-15 minutes each)

### 1. Emotion Smoothing
- **Challenge:** Current emotion detection can flicker between states. Add smoothing.
- **Hint:** Keep a history of last 5 emotions, use most common one
- **Learning:** State management, array operations

### 2. Emotion Display
- **Challenge:** Show current detected emotion as text on screen
- **Hint:** Add a div, update innerText when emotion changes
- **Learning:** DOM manipulation, UI feedback

### 3. Background Color
- **Challenge:** Change canvas background color based on emotion
- **Hint:** Use fillRect with emotion color before drawing particles
- **Learning:** Canvas rendering layers

## Intermediate Extensions (20-30 minutes each)

### 4. Particle Trails
- **Challenge:** Make particles leave trails instead of disappearing
- **Hint:** Reduce fade opacity in render() from 0.1 to 0.05
- **Learning:** Alpha compositing, visual effects

### 5. Face Position Interaction
- **Challenge:** Emit particles from detected face position on canvas
- **Hint:** MediaPipe landmarks[1] is nose tip with normalized x,y (0-1)
- **Learning:** Coordinate systems, landmark usage

### 6. Emotion History Graph
- **Challenge:** Display a bar chart showing time spent in each emotion
- **Hint:** Track timestamps, use Canvas fillRect for bars
- **Learning:** Data visualization, timing

## Advanced Extensions (45+ minutes each)

### 7. Multiple Face Support
- **Challenge:** Support 2-3 faces with different colored particles
- **Hint:** Change MediaPipe numFaces to 3, create ParticleSystem per face
- **Learning:** Multi-object tracking, instance management

### 8. Three.js 3D Particles
- **Challenge:** Replace Canvas 2D with Three.js 3D particle system
- **Hint:** Use THREE.Points with BufferGeometry, update positions in animate loop
- **Learning:** WebGL, 3D graphics, Three.js basics

### 9. Firebase Real-time Sharing
- **Challenge:** Share your current emotion with other attendees in real-time
- **Hint:** Use Firebase Realtime Database, write to /emotions/{userId}
- **Learning:** Real-time sync, Firebase integration

## Performance Challenges

### 10. FPS Counter
- **Challenge:** Display frames per second and particle count
- **Hint:** Track frame times, calculate average over 60 frames
- **Learning:** Performance monitoring

### 11. Adaptive Quality
- **Challenge:** Reduce particle count if FPS drops below 30
- **Hint:** Check deltaTime, adjust maxParticles dynamically
- **Learning:** Performance optimization, adaptive rendering

## Creative Challenges

### 12. Custom Emotions
- **Challenge:** Add new emotions like "confused", "bored", "focused"
- **Hint:** Research additional blendshape combinations
- **Learning:** Expression analysis, custom mappings

### 13. Sound Effects
- **Challenge:** Play different ambient sounds based on emotion
- **Hint:** Use Web Audio API, load audio files per emotion
- **Learning:** Audio integration, multimodal feedback

### 14. Emotion-driven Fractals
- **Challenge:** Replace particles with fractal patterns (Mandelbrot, Julia sets)
- **Hint:** Use Canvas pixel manipulation, map emotion to fractal parameters
- **Learning:** Generative art, computational art

## Documentation Challenge

### 15. Write a Tutorial
- **Challenge:** Document your extensions for other developers
- **Hint:** Include before/after screenshots, code snippets, lessons learned
- **Learning:** Technical writing, knowledge sharing

---

**Bonus:** Combine multiple extensions! Example: Emotion smoothing + Face position + Particle trails = smooth, interactive experience.

**Share:** Deploy your extended version and share the URL in workshop chat!
  </action>
  <verify>
Check EXTENSIONS.md exists and contains challenges:
- `grep -c "Challenge:" part2/face-reactive/EXTENSIONS.md` returns at least 15
- `grep -l "Beginner.*Intermediate.*Advanced" part2/face-reactive/EXTENSIONS.md`
- `wc -l part2/face-reactive/EXTENSIONS.md` shows substantial content (150+ lines)
  </verify>
  <done>
Extension challenges document exists with 15 challenges spanning beginner to advanced levels, organized by difficulty with time estimates, hints, and learning outcomes. Fast finishers have clear next steps beyond basic implementation.
  </done>
</task>

</tasks>

<verification>
Face-reactive experience project path complete when:
- [ ] Starter template exists with 60-70% scaffolding (complete infrastructure + 2 TODOs)
- [ ] Reference implementation exists with 100% working code (no TODOs)
- [ ] Extension challenges document exists with 15+ challenges at multiple difficulty levels
- [ ] All JavaScript files pass syntax validation
- [ ] Starter README.md provides clear setup and task instructions
- [ ] MediaPipe Face Landmarker integration matches research patterns from 02-RESEARCH.md
- [ ] Particle system uses object pooling (no allocations in render loop)
- [ ] Emotion detection uses threshold-based rules on 52 ARKit blendshapes
</verification>

<success_criteria>
PATH-01 requirement met when:
1. Opening starter/index.html prompts for camera access and shows blank canvas ready for implementation
2. Starter template has exactly 2 TODO sections with clear hints and time estimates (15 min + 20 min)
3. Opening reference/index.html shows working face-reactive visualization that changes based on facial expressions
4. Making different facial expressions (smile, frown, raise eyebrows) triggers visible particle changes in reference implementation
5. EXTENSIONS.md provides at least 10 additional challenges for attendees who finish early
6. Project can be completed within 60-75 minutes based on TODO time estimates (15 + 20 = 35 min implementation, 25-40 min for exploration and testing)
</success_criteria>

<output>
After completion, create `.planning/phases/02-project-paths/02-01-SUMMARY.md`
</output>
